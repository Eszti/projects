{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Unraveling the limits of large pretrained language models\n",
    "\n",
    "Manning claims about self-supervised neural learning and large pretrained language models that **\"these models assemble a broad general knowledge of the language and world to which they are exposed\"** and that **\"we are starting to see the emergence of knowledge-imbued systems that have a degree of general intelligence\"**. \n",
    "- Can we really claim that these models have a general knowledge of the world? \n",
    "- To what extent can this claim be valid?\n",
    "- What are the limits of these models, and why are they failing beyond these limits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research outline\n",
    "\n",
    "## 1. Understand the technical details\n",
    "\n",
    "- What is attention? \n",
    "- What are transformers?\n",
    "- How BERT and GTP-3 are built on the transformer architecture?\n",
    "\n",
    "## 2. Collect examples\n",
    "\n",
    "- Impressive output.\n",
    "- Some failures.\n",
    "\n",
    "## 3. Learn more about attention - key element of transformers\n",
    "\n",
    "- Why would some argue that attention is an explanation and why would others argue it is not?\n",
    "\n",
    "## 4. Dig deeper into distributional semantics - bigger context of transformers\n",
    "\n",
    "- What can we really learn with such an approach? Can we call it knowledge?\n",
    "\n",
    "## 5. Consolidate the findings\n",
    "\n",
    "- What are the limits?\n",
    "- Why are these limits reached?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant sources\n",
    "\n",
    "## 1. Understand the technical details\n",
    "\n",
    "### Papers\n",
    "\n",
    "- ✅ [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html), 2017 - Transformer model\n",
    "- [Deep Contextualized Word Representations](https://aclanthology.org/N18-1202), 2018 - ELMo, context-aware embeddings\n",
    "- ✅ [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf), 2018 - GPT model\n",
    "- ✅ [Language Models are Unsupervised Multitask Learners](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf), 2019 - GPT-2 model\n",
    "- ✅ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423), 2019 - BERT\n",
    "- ✅ [Language Models are Few-Shot Learners](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html), 2020 - GPT-3\n",
    "\n",
    "### Other sources\n",
    "\n",
    "\n",
    "\n",
    "## 2. Collect examples\n",
    "\n",
    "- Impressive output.\n",
    "- Some failures.\n",
    "\n",
    "## 3. Learn more about attention - key element of transformers\n",
    "\n",
    "- Why would some argue that attention is an explanation and why would others argue it is not?\n",
    "\n",
    "## 4. Dig deeper into distributional semantics - bigger context of transformers\n",
    "\n",
    "- What can we really learn with such an approach? Can we call it knowledge?\n",
    "\n",
    "## 5. Consolidate the findings\n",
    "\n",
    "- What are the limits?\n",
    "- Why are these limits reached?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understand the technical details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers I reviewed\n",
    "\n",
    "- [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html), 2017 - Transformer model\n",
    "- [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf), 2018 - GPT model\n",
    "- [Language Models are Unsupervised Multitask Learners](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf), 2019 - GPT-2 model\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423), 2019 - BERT\n",
    "- [Language Models are Few-Shot Learners](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html), 2020 - GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder\n",
    "\n",
    "encoder: encodes input sequence into a single vector (embedding) with RNN\n",
    "feeded into the decoder NN\n",
    "decoder generates output sequence with RNN\n",
    "\n",
    "machine-translation\n",
    "arbitrary input & output sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semi-supervised learning\n",
    "- use unlabeled data to compute word/phase-level statistics\n",
    "- use this statistics as features for a supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
